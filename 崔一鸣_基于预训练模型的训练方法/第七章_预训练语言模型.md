# 预训练语言模型  
    
**概述**  
pretrain = 让模型学习中学基础知识，夯实基本功构建基本知识体系  
finetune = 让模型学习大学专业领域更深一层的知识  
广义的预训练模型也可以指代Word2vec等静态词向量模型；以及Elmo等动态静态词向量模型  
   
**GPT & Bert**    
```
1.精调下游任务时，引入预训练的MLM、NSP Loss，可以避免"灾难性遗忘"问题；预训练任务和下游任务更加的契合 P180页      
2.对于WordPiece、WWM、N-gram不同的分词方式，仅影响预训练MLM任务；而下游任务只能使用Word_Piece分词获取输入序列 P193页   
```
备注:  
a.不同掩码策略只会影响到预训练阶段，对于下游任务精调是透明的   
b.不同掩码策略得到的Bert模型，可以无缝替换，且无需替换下游任务的任何代码   
   
**下游任务**   
1.阅读理解  
将[seq_len, embdding]通过全连接层转化为[seq_len, 1]的标量,得到起始位置的概率；同理得到终止位置的概率。  

2.序列标注  
将[seq_len, embdding]通过全连接层转化为[seq_len, K]的向量，K代表分类的概率  
注意:  
实体词和WordPiece分词是不一致的，需要将属于实体词的字词的标签设定为一致。 P207页   