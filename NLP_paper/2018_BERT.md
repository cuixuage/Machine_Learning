论文原文:https://arxiv.org/abs/1810.04805   
代码实现:https://github.com/google-research/bert   
按照原文结构记录总结  

#1.Introduction  
elmo = feature_based approach,双层双向LSTM   
GPT = fine-tuning approach,transformer decoder   
缺点: 
这两个模型left_to_right or right_to_left都是单向的contextional embedding     
bert: masked language model; 通过mask来获得其上下文相关的向量表达     
备注:  
ELMO和GPT是bert模型的基础  
  
#2. Related work  

#3.Bert  
**3.1 pretraining**
两种训练方式:  
masked LM [masked]用作于预测的单词 ; next sentence prediction(NSP)  
**3.2 fine-tuning**  
bert可以用来解决四种类型的任务  
1.句子分类  
[CLS]特殊标注在句子开头,其最终的向量表达来做为整个句子的表达。[CLS]向量通过MLP进行分类判断  
2.Token分类  
类似任务1,不过每个Token的向量表达,分别通过MLP进行分类判断  
3.推理  
两个句子的分类,[CLS]作为句子开头,[SEP]作为两个句子的分割符号。类似任务1,[CLS]是任务分类的向量表达  
4.QA问答  
后续总结...........  

#4.Reslut  
 略过  

#5.参考引用   
1.https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.8%20BERT        
2.李宏毅Bert https://www.bilibili.com/video/av64570585/       
3.bert classification,text match等等实现 https://github.com/Jiakui/awesome-bert    
4.bert代码分析https://cloud.tencent.com/developer/article/1454853  
5.bert代码分析https://zhuanlan.zhihu.com/p/58471554  


#6.代码分析  
https://github.com/google-research/bert    
**6.1 pre-training**  
通过masked_LM; next_sentence_prediction来训练bert; tokenization.py,create_pretraining_data.py,run_pretraining.py  
估计不会从头训练bert吧,代码分析跳过，后续再看吧  
https://zhuanlan.zhihu.com/p/70230267     

**6.2 fine-tuning**
1.modeling.py
attention_layer()其中from是query,to是key&value
embedding_postprocessor() positional_embedding,那么token-type-embeeding如何添加？
create_attention_mask_from_input_mask() 两个句子对的输入如何计算mask呢？
2.optimizer.py
AdmaWweightDecay,正确使用adam+L2正则的损失函数==>lazyadamW

3.file_based_input_fn_builder()  实现estimator input_fn  
4.create_model() 实现model  
5.model_fn_builder() 实现estimator model_fn  

